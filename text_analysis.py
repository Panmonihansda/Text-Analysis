# -*- coding: utf-8 -*-
"""Text Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hqw8XfnQxpfSbRSRKue-P3YU7EEaOYFF
"""

import pandas as pd
from bs4 import BeautifulSoup
from urllib.request import urlopen
import requests

#Reading the Input File
df = pd.read_csv("/content/Input.xlsx - Sheet1.csv")
df.head()

#Converting the data frame into an array so we can access everything from it
# In CSV Files you cannot access all rows
arr = df.to_numpy()
arr

#Here using beautiful Soup library I am extracting the Heading and Post info into text files
b=1
length = len(arr)
for x in range(0,length):
  url = arr[x][b]
  headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}
  rawpage = requests.get(url,headers=headers)
  soup = BeautifulSoup(rawpage.content, 'html.parser')
  content = soup.article

Heading = soup.findAll('h1',{'class':"entry-title"})  #Finds the statement which starts with h1 and class Entry title in the Javascript code
  Post = soup.findAll('div',{'class':"td-post-content"})  ##Finds the statement which starts with div and class td-post-content in the Javascript code
  str_cells = str(Heading)
  str_cell = str(Post)
  cleartext = BeautifulSoup(str_cells+str_cell, "html.parser").get_text()
  filename = "File"+str(x)
  file1 = open(filename+".txt","a")
  file1.write(cleartext)

import pandas

SW_Auditor = pandas.read_csv('StopWords_Auditor.txt',sep='delimiter', encoding = 'ISO-8859-1')
SW_Currencies = pandas.read_csv('StopWords_Currencies.txt',sep='delimiter',encoding = 'ISO-8859-1')
SW_DatesandNumbers = pandas.read_csv('StopWords_DatesandNumbers.txt',sep='delimiter',encoding = 'ISO-8859-1')
SW_Generic = pandas.read_csv('StopWords_Generic.txt',sep='delimiter', encoding = 'ISO-8859-1')
SW_GenericLong = pandas.read_csv('StopWords_GenericLong.txt',sep='delimiter', encoding = 'ISO-8859-1')
SW_Geographic = pandas.read_csv('StopWords_Geographic.txt',sep='delimiter', encoding = 'ISO-8859-1')
SW_Names = pandas.read_csv('StopWords_Names.txt',sep='delimiter', encoding = 'ISO-8859-1')

SW1 = SW_Auditor.to_numpy()
SW2 = SW_Currencies.to_numpy()
SW3 = SW_DatesandNumbers.to_numpy()
SW4 = SW_Generic.to_numpy()
SW5 = SW_GenericLong.to_numpy()
SW6 = SW_Geographic.to_numpy()
SW7 = SW_Names.to_numpy()

Stopwords_File = open("StopWords.txt", "w")
Stopwords_File.write(str(SW1))
Stopwords_File.close()

Stopwords_File = open("StopWords.txt", "a")
Stopwords_File.write(str(SW2))
Stopwords_File.close()

Stopwords_File = open("StopWords.txt", "a")
Stopwords_File.write(str(SW3))
Stopwords_File.close()

Stopwords_File = open("StopWords.txt", "a")
Stopwords_File.write(str(SW4))
Stopwords_File.close()

Stopwords_File = open("StopWords.txt", "a")
Stopwords_File.write(str(SW5))
Stopwords_File.close()

Stopwords_File = open("StopWords.txt", "a")
Stopwords_File.write(str(SW6))
Stopwords_File.close()

Stopwords_File = open("StopWords.txt", "a")
Stopwords_File.write(str(SW7))
Stopwords_File.close()

#Converting the Stopwords text file to CSV file first then to array and then finally converting it to list
import pandas
StopWordsFile = pandas.read_csv('/content/StopWords.txt',sep='delimiter', encoding = 'ISO-8859-1')
SWords = StopWordsFile.to_numpy()
StpWords = SWords.tolist()
StpWords1 = sum(StpWords, [])

#Reading the File0 in csv format converting it into an array so whole content is readable and then
#Creating a list Final0 which has all the contents of File0 which are not in StopWords File

import string
Fl0 = pd.read_csv('/content/File99.txt',sep='delimiter',encoding = 'ISO-8859-1')
FL0 = Fl0.to_numpy()
my_list = FL0.tolist() #Coverting the array to a list
Final0 = []
for word in my_list:
  if word not in StpWords1:
    Final0.append(word)

# Convert the list of lists to a single list
flat_list = sum(Final0, [])
txt = ', '.join(map(str, flat_list)) #Converts a list to a string
text = txt.replace("[", "") #replaces [ with nothing
text1 = text.replace("]","") #replaces ] with nothing
tkn = text1.split()
txn1 = ', '.join(map(str, tkn)) # String of all the words
translator = text1.maketrans('', '', string.punctuation) #Removes all the punctuations from the text1

# Use the translation table to remove all punctuation characters from the string
no_punct = text1.translate(translator)
no_punc = no_punct.replace(" ","") #Removes all the spaces from the words

#Final0 is a list of list which contains all the words which are not in Stopwords
#After this the Final0 file doesn't contain any stopwords

Pos_csvFile = pd.read_csv('/content/positive-words.txt',sep='delimiter',encoding = 'ISO-8859-1')
Pos_File = Pos_csvFile.to_numpy()
Neg_csvFile = pd.read_csv('/content/negative-words.txt',sep='delimiter',encoding = 'ISO-8859-1')
Neg_File = Neg_csvFile.to_numpy()


tokens = str(Final0).split() #Splitting the Final0 file into tokens(words)
token_sentences = text1.split('.')


No_Of_Sentences = len(token_sentences)

positive_count = 0
negative_count = 0
neutral_count = 0

Positive_Words = []
Negative_Words = []
Neutral_Words = []

#Code to count no of syllables
Scount = []
ComplexWordCount = []
def count_syllables(word):
  # Remove leading and trailing white space
  # Count the number of vowels in the word
  vowel_count = 0
  ComplexWord = 0
  for c in word:
    if c in "aeiouAEIOU":
      vowel_count += 1
      ComplexWord += 1
       # Subtract any silent vowels
  if word.endswith("e") and vowel_count > 1:
    vowel_count -= 1
  if word.endswith("le") and vowel_count > 1:
    vowel_count -= 1

  return vowel_count

word = no_punct
syllable_count = count_syllables(word)
Scount.append(syllable_count)

#Code to Store Positive,Negative and Neutral Words
for word in tokens:
  if word in Pos_File:
    Positive_Words.append(word)
  elif word in Neg_File:
    Negative_Words.append(word)
  else:
    Neutral_Words.append(word)

#Code to calculate the Positive Score, Negative Score
for token in tokens:
    # Check if the token is in the list of positive words
    if token in Positive_Words:
      # Increment the positive counter
      positive_count += 1
    elif token in Negative_Words:
      negative_count -= 1
    else:
      neutral_count += 1

negative_count*=-1

#Code to calculate the total number of mentioned pronouns
import re

def count_personal_pronouns(text):
    # Compile the regular expression
    regex = re.compile(r"\b(I|we|my|ours|us)\b(?![.]?[A-Z])")

    # Find all instances of personal pronouns in the text
    pronouns = regex.findall(text)

    # Return the number of personal pronouns found
    return len(pronouns)

# Example usage
print(f"Number of personal pronouns: {count_personal_pronouns(str(Final0))}")

#Calculating Total number of words
Total_words=0
for i in tokens:
  cur_word = len(i)
  Total_words+=cur_word
print("Total No Of Words:",Total_words)

sum_of_wordcount = Total_words / len(tokens)

positive_score = positive_count
negative_score = negative_count
polarity_score = (positive_score-negative_score)/((positive_score + negative_score) + 0.000001)
subjectivity_score = (positive_score+negative_score)/((len(tokens)) + 0.000001)
# Calculate the average sentence length
avg_sentence_length = len(no_punc) / len(token_sentences)
# Calculate the average number of words per sentence
avg_words_per_sentence = len(tokens) / len(token_sentences)

num_complex_words = 0
Percentage_of_Complex_words = (num_complex_words / Total_words)
Fog_Index = 0.4 * (avg_words_per_sentence + Percentage_of_Complex_words)
WordCount = len(no_punc)

print(f"Positive score: {positive_score}")
print(f"Negative score: {negative_score}")
print(f"Polarity score: {polarity_score}")
print(f"Subjectivity_score: {subjectivity_score}")
print(f"Average Sentence Length: {avg_sentence_length}")
print(f"Average Number of Words Per Sentence: {avg_words_per_sentence}")
print(f"Fog_Index: {Fog_Index}")
print(f"WordCount: {WordCount}")
print("Syllable count per word:",syllable_count)
print("Average Word Length:",sum_of_wordcount)

#Here we are creating a CSV file where we append the column names and we are adding one row
import csv

# Create a list of rows to be written to the CSV file
rows = [
    ['URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'FOG INDEX', 'AVG NUMBER O FWORDS PER SENTENCE','WORDCOUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'],
    [arr[0][1],positive_score, negative_score , polarity_score, subjectivity_score, avg_sentence_length, Fog_Index, avg_words_per_sentence, WordCount, syllable_count, count_personal_pronouns(str(Final0)), sum_of_wordcount]]

# Open the file in append mode
with open('People.csv', 'a', newline='') as csvfile:
    # Create a CSV writer
    writer = csv.writer(csvfile)

    # Write each row to the CSV file
    for row in rows:
        writer.writerow(row)

#Later we use this code to append the row values to the csv file
import csv

# Open the file in append mode
with open('People.csv', 'a', newline='') as csv_file:
  # Create a writer object
  csv_writer = csv.writer(csv_file)

  # Write a row to the CSV file
  csv_writer.writerow([arr[99][1],positive_score, negative_score , polarity_score, subjectivity_score, avg_sentence_length, Fog_Index, avg_words_per_sentence, WordCount, syllable_count, count_personal_pronouns(str(Final0)), sum_of_wordcount])

import pandas as pd

df = pd.read_csv("People.csv")
df.head()

